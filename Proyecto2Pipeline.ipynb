{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Proyecto 1 (Etapa 2) - Generación de Pipelines\n",
        "\n",
        "En este caso empezaremos mostrando lo que se realizó en el proyecto 1, dejando en claro que usaremos los modelos ya hechos en la etapa anterior para ser automatizados mediante pipelines. Así que después utilizaremos un modelo de TfidVectorizer para el manejo del texto y empezaremos con la generación del pipelines para el proyecto 1 parte 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4644f48b"
      },
      "source": [
        "### Proyecto 1 (Etapa 1) - Elegibilidad\n",
        "\n",
        "#### Determinar la elegibilidad de un paciente para ensayos clínicos de cáncer a partir de texto descriptivo\n",
        "  * Jairo Cespedes - 201912008\n",
        "  * Daniel Alejandro Ángel Fuertes - 201911345\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ef005b7",
        "jp-MarkdownHeadingCollapsed": true,
        "tags": []
      },
      "source": [
        "## Modelos propuestos\n",
        "\n",
        " * Árboles de Decisión (Hecho por ambos)\n",
        " * Random Forest \n",
        " * Regresión logística"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A8hXGJU0SKNO"
      },
      "source": [
        "# 1. Importación de librerías \n",
        "\n",
        "Para la realización de modelos y limpieza de datos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "aD7WCMKkSKNP"
      },
      "outputs": [],
      "source": [
        "# Tratamiento de datos\n",
        "# ==============================================================================\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import string\n",
        "import re\n",
        "\n",
        "# Gráficos\n",
        "# ==============================================================================\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import style\n",
        "import seaborn as sns\n",
        "#style.use('ggplot') or plt.style.use('ggplot')\n",
        "\n",
        "# Preprocesado y modelado\n",
        "# ==============================================================================\n",
        "from sklearn import svm\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "# Para preparar los datos\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "# Para crear el arbol de decisión \n",
        "from sklearn.tree import DecisionTreeClassifier \n",
        "# Para realizar la separación del conjunto de aprendizaje en entrenamiento y test.\n",
        "from sklearn.model_selection import train_test_split\n",
        "# Para evaluar el modelo\n",
        "from sklearn.metrics import confusion_matrix, classification_report, precision_score, recall_score, f1_score, accuracy_score\n",
        "from sklearn.metrics import plot_confusion_matrix\n",
        "# Para búsqueda de hiperparámetros\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "# Para la validación cruzada\n",
        "from sklearn.model_selection import KFold \n",
        "#Librerías para la visualización\n",
        "import matplotlib as mplt\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "#Regresión logística\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn import metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "n_EfLudSSKNR"
      },
      "outputs": [],
      "source": [
        "# Se cargan los datos. \n",
        "data=pd.read_csv('clinical_trials_on_cancer_data_clasificacion.csv', sep=',', encoding = 'utf-8')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "label                  0\n",
              "study_and_condition    0\n",
              "dtype: int64"
            ]
          },
          "execution_count": 50,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "UTYP-JONSKNR",
        "outputId": "6807443b-f9c8-43b2-d00d-8758994ce7d5"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>study_and_condition</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>__label__0</td>\n",
              "      <td>study interventions are Saracatinib . recurren...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>__label__1</td>\n",
              "      <td>study interventions are Stem cell transplantat...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>__label__0</td>\n",
              "      <td>study interventions are Lenograstim . recurren...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>__label__0</td>\n",
              "      <td>study interventions are Doxorubicin . stage ii...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>__label__1</td>\n",
              "      <td>study interventions are Poly I-C . prostate ca...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        label                                study_and_condition\n",
              "0  __label__0  study interventions are Saracatinib . recurren...\n",
              "1  __label__1  study interventions are Stem cell transplantat...\n",
              "2  __label__0  study interventions are Lenograstim . recurren...\n",
              "3  __label__0  study interventions are Doxorubicin . stage ii...\n",
              "4  __label__1  study interventions are Poly I-C . prostate ca..."
            ]
          },
          "execution_count": 51,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# sacando valores nulos para evitar errores\n",
        "data.dropna(inplace = True)\n",
        "\n",
        "data.head()\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "qVC_wQ_fSKNS",
        "outputId": "dcc13e01-4b76-4f7d-973c-a938a898bed4"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>study_and_condition</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>study interventions are Saracatinib . recurren...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>study interventions are Stem cell transplantat...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>study interventions are Lenograstim . recurren...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>study interventions are Doxorubicin . stage ii...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>study interventions are Poly I-C . prostate ca...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  label                                study_and_condition\n",
              "0     0  study interventions are Saracatinib . recurren...\n",
              "1     1  study interventions are Stem cell transplantat...\n",
              "2     0  study interventions are Lenograstim . recurren...\n",
              "3     0  study interventions are Doxorubicin . stage ii...\n",
              "4     1  study interventions are Poly I-C . prostate ca..."
            ]
          },
          "execution_count": 52,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Para dejar el label como número binario 0 o 1\n",
        "data.loc[(data['label'] == '__label__0'), 'label'] = 0\n",
        "data.loc[(data['label'] == '__label__1'), 'label'] = 1\n",
        "\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "a6_6vGRvSKNT"
      },
      "outputs": [],
      "source": [
        "# Función tomada y adaptada de: https://www.cienciadedatos.net/documentos/py25-text-mining-python.html\n",
        "def limpiar(texto):\n",
        "    '''\n",
        "    Esta función limpia el texto de diferentes palabras y caracteres que puedan no afectar el modelo\n",
        "    El orden en el que se va limpiando el texto no es arbitrario.\n",
        "    El listado de signos de puntuación se ha obtenido de: print(string.punctuation)\n",
        "    y re.escape(string.punctuation)\n",
        "    '''\n",
        "    \n",
        "    # Se convierte todo el texto a minúsculas\n",
        "    nuevo_texto = texto.lower()\n",
        "    # Eliminación de signos de puntuación\n",
        "    regex = '[\\\\!\\\\\"\\\\#\\\\$\\\\%\\\\&\\\\\\'\\\\(\\\\)\\\\*\\\\+\\\\,\\\\-\\\\.\\\\/\\\\:\\\\;\\\\<\\\\=\\\\>\\\\?\\\\@\\\\[\\\\\\\\\\\\]\\\\^_\\\\`\\\\{\\\\|\\\\}\\\\~]'\n",
        "    nuevo_texto = re.sub(regex , ' ', nuevo_texto)\n",
        "    # Eliminación de conjunciones y palabras innecesarias\n",
        "    conj = [\" and \",\" are \",\" for \",\" of \",\" with \",\" to \",\" as \",\" or \",\" do \",\" the \",\" in \",\" than \", \"study \", \" interventions \"]\n",
        "    for i in conj:\n",
        "        nuevo_texto = re.sub(i, ' ', nuevo_texto)\n",
        "    # Eliminación de números\n",
        "    nuevo_texto = re.sub(\"\\d+\", ' ', nuevo_texto)\n",
        "    # Eliminación de espacios en blanco múltiples\n",
        "    nuevo_texto = re.sub(\"\\\\s+\", ' ', nuevo_texto)\n",
        "\n",
        "    return(nuevo_texto)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "aeTjUvEFSKNU",
        "outputId": "8eade0e6-ceab-4752-fa1c-da7d17ce5ab6"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>study_and_condition</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>saracatinib recurrent verrucous carcinoma lar...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>stem cell transplantation hodgkin lymphoma di...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>lenograstim recurrent adult diffuse mixed cel...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>doxorubicin stage iii diffuse large cell lymp...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>poly i c prostate cancer diagnosis unresolved...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  label                                study_and_condition\n",
              "0     0   saracatinib recurrent verrucous carcinoma lar...\n",
              "1     1   stem cell transplantation hodgkin lymphoma di...\n",
              "2     0   lenograstim recurrent adult diffuse mixed cel...\n",
              "3     0   doxorubicin stage iii diffuse large cell lymp...\n",
              "4     1   poly i c prostate cancer diagnosis unresolved..."
            ]
          },
          "execution_count": 54,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Se utiliza la función para limpiar el texto\n",
        "data[\"study_and_condition\"]= data['study_and_condition'].apply(lambda text: limpiar(text))\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "NM-7Vc7tSKNU"
      },
      "outputs": [],
      "source": [
        "#Se pasa la columna label a valores numéricos\n",
        "data[\"label\"] = data[\"label\"].astype('float64')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2-MSvygbSKNV"
      },
      "source": [
        "# 2. Modelo de TfidVectorizer (Cambiado ya que para hacer el pipeline no es muy recomendable usar bag of words)\n",
        "\n",
        "Se utliza un modelo de TfidVectorizer para poder crear una matriz de TF-IDF features. para las palabras dentro del dataframe y así cambiarlo a una representación numérica para poder utilizar modelos de clasificación en él"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "w7HeRMPASKNV"
      },
      "outputs": [],
      "source": [
        "vectorizer = TfidfVectorizer(lowercase=False)\n",
        "#Se utiliza el modelo en los textos dados\n",
        "label = vectorizer.fit_transform(data[\"study_and_condition\"])\n",
        "#print(vectorizer.get_feature_names())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bl7a8gn1SKNV"
      },
      "source": [
        "# 3. Construcción del modelo - árbol de decisión\n",
        "\n",
        "Después de limpiar y discriminar los datos, se procede a crear el árbol de decisión (para generar puntos de decisión de clasificación). "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "DgvMkQyGSKNW"
      },
      "outputs": [],
      "source": [
        "# Se selecciona la variable objetivo, en este caso \"label\".\n",
        "Y = data['label']\n",
        "# label of words\n",
        "X = label.toarray()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "Ps7gegXSSKNW"
      },
      "outputs": [],
      "source": [
        "# Dividir los datos en entrenamiento y test\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Crear el objeto de arbol de decisión. Utilicemos como criterio de pureza la entropía.\n",
        "arbol = DecisionTreeClassifier(criterion='entropy', random_state=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Entrenar el modelo de arbol de decisión con los datos de entrenamiento\n",
        "arbol = arbol.fit(X_train,Y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Con el árbol obtenido calculamos la exactitud sobre el entrenamiento y sobre el test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Exactitud sobre entrenamiento: 1.00\n",
            "Exactitud sobre test: 0.78\n"
          ]
        }
      ],
      "source": [
        "# Obtener el mejor modelo.\n",
        "#arbol_final = mejor_modelo.best_estimator_\n",
        "arbol_final= arbol\n",
        "# Probemos ahora este modelo sobre test.\n",
        "y_pred_train = arbol_final.predict(X_train)\n",
        "y_pred_test = arbol_final.predict(X_test)\n",
        "print('Exactitud sobre entrenamiento: %.2f' % accuracy_score(Y_train, y_pred_train))\n",
        "print('Exactitud sobre test: %.2f' % accuracy_score(Y_test, y_pred_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.79      0.77      0.78      1228\n",
            "         1.0       0.77      0.79      0.78      1172\n",
            "\n",
            "    accuracy                           0.78      2400\n",
            "   macro avg       0.78      0.78      0.78      2400\n",
            "weighted avg       0.78      0.78      0.78      2400\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(classification_report(Y_test, y_pred_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cc3e5d12-1938-4883-acf9-eeb48d4f9b04"
      },
      "source": [
        "# 4. Random Forest Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b70e3c86-866d-4440-913e-1ddffa69fff9"
      },
      "source": [
        "Calculamos varios árboles no correlacionados para luego promediarlos y poder obtener los valores deseados, este fue el modelo elegido debido a que es el que mayor accuracy ofrece de los ya mostrados en la etapa 1 (Árbol de decisión, Regresión logística y Random Forest)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "executionInfo": {
          "elapsed": 55,
          "status": "aborted",
          "timestamp": 1630717479626,
          "user": {
            "displayName": "Andres Ochoa",
            "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj8Z1R9xxNMaekT5DNe2l5_qZwBTQyYAnr22xmEyg=s64",
            "userId": "05605349094351429294"
          },
          "user_tz": 300
        },
        "id": "81d5d896-4c05-422f-9fee-1d152f27a1cc"
      },
      "outputs": [],
      "source": [
        "#Se importa un modelo de Random Forest\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "#Import scikit-learn metrics module for accuracy calculation\n",
        "from sklearn import metrics\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2) # 80% de entrenamiento y 20% de test                "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Se crea un clasificador Gaussiano\n",
        "clf=RandomForestClassifier(n_estimators=100)\n",
        "#Se hace la prediccion del modelo\n",
        "clf.fit(X_train,y_train)\n",
        "y_pred=clf.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.8220833333333334\n"
          ]
        }
      ],
      "source": [
        "# Accuracy del modelo\n",
        "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "executionInfo": {
          "elapsed": 58,
          "status": "aborted",
          "timestamp": 1630717479630,
          "user": {
            "displayName": "Andres Ochoa",
            "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj8Z1R9xxNMaekT5DNe2l5_qZwBTQyYAnr22xmEyg=s64",
            "userId": "05605349094351429294"
          },
          "user_tz": 300
        },
        "id": "2805146a-122e-42bd-9079-a023fa77eee1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.84      0.81      0.82      1228\n",
            "         1.0       0.80      0.84      0.82      1172\n",
            "\n",
            "    accuracy                           0.82      2400\n",
            "   macro avg       0.82      0.82      0.82      2400\n",
            "weighted avg       0.82      0.82      0.82      2400\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(classification_report(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 5. Regresión logística\n",
        "Se ejecuta el modelo de Regresión Logística y sus respectivas estadísticas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LogisticRegression Accuracy 0.807\n"
          ]
        }
      ],
      "source": [
        "#\n",
        "# Create an instance of LogisticRegression classifier\n",
        "#\n",
        "lr = LogisticRegression(random_state=0, max_iter=1000)\n",
        "#\n",
        "# Fit the model\n",
        "#\n",
        "lr.fit(X_train, y_train)\n",
        "#\n",
        "# Create the predictions\n",
        "#\n",
        "y_predict = lr.predict(X_test)\n",
        "  \n",
        "# Use metrics.accuracy_score to measure the score\n",
        "print(\"LogisticRegression Accuracy %.3f\" %metrics.accuracy_score(y_test, y_predict))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.82      0.80      0.81      1228\n",
            "         1.0       0.79      0.82      0.81      1172\n",
            "\n",
            "    accuracy                           0.81      2400\n",
            "   macro avg       0.81      0.81      0.81      2400\n",
            "weighted avg       0.81      0.81      0.81      2400\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(classification_report(y_test, y_predict))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 6. Exportar datos limpios\n",
        "Se exporta el dataframe modificado"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {},
      "outputs": [],
      "source": [
        "feature_names =vectorizer.get_feature_names()\n",
        "#Se pasa la columna label a valores numéricos\n",
        "data[\"label\"] = data[\"label\"].astype('int64')\n",
        "# Create data frame\n",
        "df=pd.DataFrame(label.toarray(), columns=feature_names)\n",
        "data.to_csv('DatosModificados.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Inicio Etapa 2\n",
        "En esta sección se realizará el pipeline inspirándose en la limpieza de datos mostrada anteriormente (Con el uso de la función de limpieza de texto, el modelo de TfidVectorizer y los modelos mostrados anteriormente). Todo esto nos permitirá automatizar los modelos junto a su respectiva limpieza y preparación."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Realizamos el acceso a librerías adicionales para la creación de pipelines\n",
        "from sklearn.preprocessing import StandardScaler, PowerTransformer\n",
        "from sklearn.compose import TransformedTargetRegressor\n",
        "from sklearn.pipeline import FeatureUnion, Pipeline, make_pipeline\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.compose import ColumnTransformer\n",
        "import joblib\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler, FunctionTransformer\n",
        "from joblib import load\n",
        "#Se importa un modelo de Random Forest\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "#Import scikit-learn metrics module for accuracy calculation\n",
        "from sklearn import metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Se selecciona la variable objetivo, en este caso \"label\".\n",
        "Y = data['label']\n",
        "# label of words\n",
        "X = data['study_and_condition']\n",
        "## Dividir los datos en entrenamiento y test\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=0)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Ya en este momento creamos el pipeline con el TfidVectorizer y los modelos ya desarrollados y ajustados en la etapa 1 del proyecto, además quitamos las palabras que no sirven para el modelo como se explicó anteriormente"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Creación pipeline\n"
          ]
        }
      ],
      "source": [
        "print(\"Creación pipeline\")\n",
        "pipeForest = Pipeline(steps=[                      \n",
        "                       ('data_preprocessing',TfidfVectorizer(analyzer='word',stop_words=set([\" and \",\" are \",\" for \",\" of \",\" with \",\" to \",\" as \",\" or \",\" do \",\" the \",\" in \",\" than \", \"study \", \" interventions \"]))),\n",
        "                       ('rf', clf)\n",
        "])\n",
        "pipeRL = Pipeline(steps=[                      \n",
        "                       ('data_preprocessing',TfidfVectorizer(analyzer='word',stop_words=set([\" and \",\" are \",\" for \",\" of \",\" with \",\" to \",\" as \",\" or \",\" do \",\" the \",\" in \",\" than \", \"study \", \" interventions \"]))),\n",
        "                       ('regresion_model', lr)\n",
        "])\n",
        "pipeArbol = Pipeline(steps=[                      \n",
        "                       ('data_preprocessing',TfidfVectorizer(analyzer='word', stop_words=set([\" and \",\" are \",\" for \",\" of \",\" with \",\" to \",\" as \",\" or \",\" do \",\" the \",\" in \",\" than \", \"study \", \" interventions \"]))),\n",
        "                       ('arbol_model', arbol)\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "D:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['and', 'are', 'as', 'do', 'for', 'in', 'interventions', 'of', 'or', 'study', 'than', 'the', 'to', 'with'] not in stop_words.\n",
            "  warnings.warn('Your stop_words may be inconsistent with '\n",
            "D:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['and', 'are', 'as', 'do', 'for', 'in', 'interventions', 'of', 'or', 'study', 'than', 'the', 'to', 'with'] not in stop_words.\n",
            "  warnings.warn('Your stop_words may be inconsistent with '\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "['assets/modeloP1E2Arbol.pkl']"
            ]
          },
          "execution_count": 105,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Ajustamos los valores del pipeline a los valores de entrenamiento\n",
        "pipeForest.fit(X_train,Y_train)\n",
        "pipeRL.fit(X_train,Y_train)\n",
        "pipeArbol.fit(X_train,Y_train)\n",
        "# Guardar nuestra canalización en un archivo pickle binario \n",
        "joblib.dump(pipeForest, 'assets/modeloP1E2Forest.pkl')\n",
        "joblib.dump(pipeRL, 'assets/modeloP1E2Regresion.pkl')\n",
        "joblib.dump(pipeArbol, 'assets/modeloP1E2Arbol.pkl')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finalmente, realizamos la prueba de si funciona el modelo exportado y se calcula su Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "D:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['and', 'are', 'as', 'do', 'for', 'in', 'interventions', 'of', 'or', 'study', 'than', 'the', 'to', 'with'] not in stop_words.\n",
            "  warnings.warn('Your stop_words may be inconsistent with '\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model Accuracy 0.830\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.81      0.83      1228\n",
            "           1       0.81      0.85      0.83      1172\n",
            "\n",
            "    accuracy                           0.83      2400\n",
            "   macro avg       0.83      0.83      0.83      2400\n",
            "weighted avg       0.83      0.83      0.83      2400\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#Cargamos el modelo de Random Forest\n",
        "modelR = load(\"assets/modeloP1E2Forest.pkl\")\n",
        "#Se realiza la predicción con el modelo\n",
        "y_predict = modelR.predict(X_test)\n",
        "# Usar las métricas para analizar el modelo\n",
        "print(\"Model Accuracy %.3f\" %metrics.accuracy_score(Y_test, y_predict))\n",
        "print(classification_report(Y_test, y_predict))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model Accuracy 0.825\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.81      0.83      1228\n",
            "           1       0.81      0.84      0.82      1172\n",
            "\n",
            "    accuracy                           0.82      2400\n",
            "   macro avg       0.82      0.82      0.82      2400\n",
            "weighted avg       0.83      0.82      0.82      2400\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#Cargamos el modelo de Regresión Logística\n",
        "modelRL = load(\"assets/modeloP1E2Regresion.pkl\")\n",
        "#Se realiza la predicción con el modelo\n",
        "y_predict = modelRL.predict(X_test)\n",
        "# Usar las métricas para analizar el modelo\n",
        "print(\"Model Accuracy %.3f\" %metrics.accuracy_score(Y_test, y_predict))\n",
        "print(classification_report(Y_test, y_predict))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model Accuracy 0.767\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.78      0.75      0.77      1228\n",
            "           1       0.75      0.78      0.77      1172\n",
            "\n",
            "    accuracy                           0.77      2400\n",
            "   macro avg       0.77      0.77      0.77      2400\n",
            "weighted avg       0.77      0.77      0.77      2400\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#Cargamos el modelo de Arbol de Decisión\n",
        "modelA = load(\"assets/modeloP1E2Arbol.pkl\")\n",
        "#Se realiza la predicción con el modelo\n",
        "y_predict = modelA.predict(X_test)\n",
        "# Usar las métricas para analizar el modelo\n",
        "print(\"Model Accuracy %.3f\" %metrics.accuracy_score(Y_test, y_predict))\n",
        "print(classification_report(Y_test, y_predict))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Proyecto1SK (1).ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "9387e9e37a0af8fff0fcbade4737076ebc87d0c8fec44545ee3d2fd05cdc73df"
    },
    "kernelspec": {
      "display_name": "Python 3.8.5 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
